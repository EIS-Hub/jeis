{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "px = 1/plt.rcParams[\"figure.dpi\"]\n",
    "\n",
    "import os\n",
    "\n",
    "# set how much memory in your local CPU/GPU will be pre-allocated for JAX\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".75\"\n",
    "\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset, already flat and normalized\n",
    "X_train_1 = jnp.load('../mnist_np/X_train_1.npy') \n",
    "X_train_2 = jnp.load('../mnist_np/X_train_2.npy') \n",
    "X_train_3 = jnp.load('../mnist_np/X_train_3.npy') \n",
    "X_train_4 = jnp.load('../mnist_np/X_train_4.npy') \n",
    "# create X_train out of 4 X_trains\n",
    "X_train = jnp.concatenate([X_train_1, X_train_2, X_train_3, X_train_4], axis=0)\n",
    "y_train = jnp.load('../mnist_np/y_train.npy')\n",
    "X_test = jnp.load('../mnist_np/X_test.npy')\n",
    "y_test = jnp.load('../mnist_np/y_test.npy')\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the data in batches of 2000 (going above take more time)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n"
     ]
    }
   ],
   "source": [
    "def rate_encoding(key, X, sim_len=100):\n",
    "    \n",
    "    def bernoulli_encoding(key, spike_trains, sim_len):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        return key, jax.random.bernoulli(key, spike_trains, (sim_len, spike_trains.shape[0], spike_trains.shape[1]))\n",
    "    \n",
    "    print('Encoding the data in batches of 2000 (going above take more time)')\n",
    "    X_encoded = []\n",
    "    batch_size = 2000\n",
    "    for i in range(X.shape[0]//batch_size):\n",
    "        key, X_encoded_ = bernoulli_encoding(key, X[i*batch_size:(i+1)*batch_size], sim_len=100)\n",
    "        print(X_encoded_.shape)\n",
    "        X_encoded.append(X_encoded_)\n",
    "\n",
    "    return key, jnp.concatenate(X_encoded, axis=1)\n",
    "\n",
    "# do rate encoding on X_test\n",
    "key = jax.random.PRNGKey(9)\n",
    "key, X_test_encoded = rate_encoding(key, X_test, sim_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the data in batches of 2000 (going above take more time)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n"
     ]
    }
   ],
   "source": [
    "# do rate encoding on Xtrain\n",
    "key, X_train_encoded = rate_encoding(key, X_train, sim_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.custom_jvp\n",
    "def gr_than(x, thr):\n",
    "    return (x > thr).astype(jnp.float32)\n",
    "\n",
    "@gr_than.defjvp\n",
    "def gr_jvp(primals, tangents):\n",
    "    x, thr = primals\n",
    "    x_dot, thr_dot = tangents\n",
    "    primal_out = gr_than(x, thr)\n",
    "    tangent_out = x_dot * 1 / (jnp.absolute(x-thr)+1)**2\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "# TODO 1: Implement the neuron model\n",
    "# NOTE : The neuron model is a LIF neuron model, its input are a state object and input spikes\n",
    "#        The state object contains the following:\n",
    "#        - state[0]: w, out_spikes, I_in, V_mem\n",
    "#        - state[1]: tau_mem, V_th, timestep\n",
    "#        The neuron model should return the updated state object and the output values (I_in, V_mem, out_spikes)\n",
    "# NOTE: V_mem should be constrained to be positive\n",
    "def lif_forward(state, input_spikes):\n",
    "    '''\n",
    "    TODO 1.1: complete description\n",
    "    :param state: \n",
    "        - state[0]: w, out_spikes, I_in, V_mem\n",
    "            - w: learnable weights of the layer, shape = (784, layer_dim), dtype = float32\n",
    "\t\t\t\t\t\t- out_spike: output of the LIF neuron, shape = (784,), dtype = int\n",
    "\t\t\t\t\t\t- I_in: input current into the LIF neuron, shape = (784,), dtype = float32\n",
    "\t\t\t\t\t\t- V_mem: (non-neg) membrane potential of the LIF neuron, shape = (784,), dtype = float32\n",
    "\t\t\t\t- state[1]: tau_mem, V_th, timestep\n",
    "\t\t\t\t\t\t- tau_mem: time constant of the neuron model, shape = (1,1), dtype = (float32)\n",
    "\t\t\t\t\t\t- V_th: threshold membrane potential, shape = (1,1), dtype = float32\n",
    "\t\t\t\t\t\t- timestep: time delta, shape = (1,1), dtype = float32\n",
    "    :return: \n",
    "\t\t- state:\n",
    "\t\t\t- state[0]: w. out_spikes, I_in, V_mem\n",
    "\t\t\t\t\tupdated values for w, out_spikes depending on I_in, V_mem and input_spikes\n",
    "\t\t\t- state[1]: tau_mem, V_th, timestep\n",
    "\t\t\t\t\tauxiliary, fixed variables of the neuron model\n",
    "\t\t- output: printable output values\n",
    "\t\t\t- I_in\n",
    "\t\t\t- V_mem\n",
    "\t\t\t- out_spikes\n",
    "\t\t\t\t\t\n",
    "    '''\n",
    "    w, out_spikes, I_in, V_mem = state[0]\n",
    "    tau_mem, Vth, timestep = state[1]\n",
    "    # TODO 1.2: Implement the LIF neuron model using jnp functions where possible\n",
    "    I_in = jnp.dot(w, input_spikes)\n",
    "    V_mem = (1 - timestep/tau_mem) * V_mem + I_in - out_spikes * Vth\n",
    "    # constraining V_mem to be non-negative\n",
    "    V_mem = jnp.maximum(0, V_mem)\n",
    "    out_spikes = gr_than(V_mem, Vth)\n",
    "\n",
    "    # TODO 1.3: return the updated state and the output values\n",
    "    return ((w, out_spikes, I_in, V_mem), state[1]), (I_in, V_mem, out_spikes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomWeightInit(parent_key, scale, in_width, out_width):\n",
    "    # TODO 2: implement the random weight initialization function using rax.random.split() and keys\n",
    "    in_width = in_width\n",
    "    out_width = out_width\n",
    "    weight_key, bias_key = jax.random.split(parent_key)\n",
    "    W = scale*jax.random.normal(weight_key, shape=(out_width, in_width)) # random init of [weights, biases] tuple for each layer\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: implement loss function for a single image\n",
    "# cross entropy loss of a batch of input, returns loss and accuracy, also calculates prediction\n",
    "def loss_fn_per_image(W, static_params, img, lbl):\n",
    "    # TODO 3.1: define V_mem, I_in, out_spikes, state\n",
    "    num_classes = W.shape[0]\n",
    "    V_mem = jnp.zeros((num_classes,), dtype='float32')\n",
    "    I_in = jnp.zeros((num_classes,), dtype='float32')\n",
    "    out_spikes = jnp.zeros((num_classes,), dtype='float32')\n",
    "\n",
    "    state = ((W, out_spikes, I_in, V_mem), static_params)\n",
    "    \n",
    "    # # TODO 3.2: define V_mem_data using jnp.zeros()\n",
    "    # V_mem_data = jnp.zeros((img.shape[0], num_classes), dtype='float32')\n",
    "    # # TODO 3.3: iterate through the timesteps and call lif_forward() for each timestep, save V_mem_data\n",
    "    # for i in range(img.shape[0]):\n",
    "    #     state, plot_values = lif_forward(state, img[i])\n",
    "    #     # TODO 3.4: use JAX's .at[].set() to update V_mem_data\n",
    "    #     V_mem_data.at[i, :].set(plot_values[1])\n",
    "\n",
    "    # TODO LATER: replace for-loop with jax.lax.scan() to speed up\n",
    "    state, plot_values = jax.lax.scan(lif_forward,state,img)   \n",
    "    V_mem_data = plot_values[1]\n",
    "\n",
    "\n",
    "    # TODO 3.5: calculate prediction using V_mem_data\n",
    "    # we define prediction for MNIST to be the highest occurring membrane voltage across all timesteps\n",
    "    max_per_class = V_mem_data.max(axis=0)\n",
    "    prediction = max_per_class.argmax()\n",
    "\n",
    "    # TODO 3.6: define logits and loss, use softmax() function for loss\n",
    "    logits = jax.nn.softmax(max_per_class)\n",
    "    loss = -jnp.mean(jnp.log(logits[prediction]))\n",
    "\n",
    "    # TODO 3.7: define accuracy, and use jnp.where() for it\n",
    "    acc = jnp.where(prediction == lbl, 1.0, 0.0)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: implement the training function\n",
    "def train_one_epoch(W, static_params, input):\n",
    "    # TODO 4.1: unpack input to img_batch, lbl_batch, (input comes in as a tuple of (img_batch, lbl_batch))\n",
    "    img_batch, lbl_batch = input\n",
    "    \n",
    "    # TODO 4.2: iterate through all batches in the dataset, call loss_fn_per_image() for each image in the batch, calculate loss and accuracy\n",
    "    # batch_loss = []\n",
    "    # batch_acc = []\n",
    "\n",
    "    # for i in range(img_batch.shape[0]):\n",
    "    #     # TODO 4.2.1: call loss_fn_per_image() for each image in the batch\n",
    "    #     loss, acc = loss_fn_per_image(W, static_params, img_batch[i], lbl_batch[i])\n",
    "    #     # TODO 4.2.2: append loss, acc to batched_loss, batched_acc\n",
    "    #     batch_loss.append(loss)\n",
    "    #     batch_acc.append(acc)\n",
    "    \n",
    "    # TODO LATER: replace for-loop with jax.vmap for speed-up\n",
    "    batch_loss, batch_acc = jax.vmap(loss_fn_per_image, in_axes=(None, None, 0, 0))(W, static_params, img_batch, lbl_batch)\n",
    "    \n",
    "    \n",
    "    # TODO 4.3: stack the batched loss and accuracy\n",
    "    batch_loss = jnp.stack(batch_loss)\n",
    "    batch_acc = jnp.stack(batch_acc)\n",
    "\n",
    "    # TODO 4.4: calculate the mean loss and accuracy for the batch\n",
    "    epoch_loss = jnp.mean(batch_loss)\n",
    "    epoch_acc = jnp.mean(batch_acc)\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset():\n",
    "    def __init__(self, X_images,Y_labels):\n",
    "        self.imgs = X_images\n",
    "        self.lbls = Y_labels\n",
    "    def __len__(self): # return length of dataset, i.e. nb of MNIST pictures\n",
    "        return self.imgs.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[:,idx,:], self.lbls[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test functionality of MNISTDataset() class \n",
    "train_dataset = MNISTDataset(X_train_encoded, y_train)\n",
    "test_dataset = MNISTDataset(X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    transposed_data = list(zip(*batch)) # *() unpacks data !\n",
    "\n",
    "    labels= np.array(transposed_data[1])\n",
    "    imgs = np.stack(transposed_data[0])\n",
    "\n",
    "    return imgs, labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: Implement the forward loop: \n",
    "# 5.1 Initialize all necessary state variables and parameters\n",
    "# 5.2 Iterate over the training data for one epoch (for now)\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# TODO 5.1: initialize state variables of LIF neuron for every time step\n",
    "\n",
    "# initialize dynamic params\n",
    "seed = 9\n",
    "parent_key = jax.random.PRNGKey(seed)\n",
    "# TODO 5.1.1: initialize weights, think about dimensions, scale with 0.03\n",
    "W = randomWeightInit(parent_key, 0.03, 784, 10) \n",
    "tau_mem = 10e-3\n",
    "V_th = 1.0\n",
    "timestep = 1e-3\n",
    "\n",
    "# TODO 5.2: define static_params\n",
    "static_params = (tau_mem, V_th, timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 1.615514874458313, Accuracy: 0.0546875\n",
      "Epoch: 0, Batch: 10, Loss: 1.638960599899292, Accuracy: 0.09375\n",
      "Epoch: 0, Batch: 20, Loss: 1.6376497745513916, Accuracy: 0.109375\n",
      "Epoch: 0, Batch: 30, Loss: 1.62709641456604, Accuracy: 0.109375\n",
      "Epoch: 0, Batch: 40, Loss: 1.645958662033081, Accuracy: 0.0625\n",
      "Epoch: 0, Batch: 50, Loss: 1.6312494277954102, Accuracy: 0.09375\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # TODO 5.3: use train_loader to iterate over the dataset\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # TODO 5.3.1: get the data from the DataLoader, save in img_batch, lbl_batch\n",
    "        img_batch, lbl_batch = data\n",
    "        loss, acc = train_one_epoch(W, static_params, (img_batch, lbl_batch))\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {i}, Loss: {loss}, Accuracy: {acc}')\n",
    "        # stop after 50 batches (non-JAX version will take a while!)\n",
    "        if i == 50:\n",
    "            break\n",
    "\n",
    "print('DONE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
