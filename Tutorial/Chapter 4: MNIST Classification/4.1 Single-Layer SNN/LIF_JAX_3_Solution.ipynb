{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "px = 1/plt.rcParams[\"figure.dpi\"]\n",
    "\n",
    "import os\n",
    "\n",
    "# set how much memory in your local CPU/GPU will be pre-allocated for JAX\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".75\"\n",
    "\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset, already flat and normalized\n",
    "X_train_1 = jnp.load('../mnist_np/X_train_1.npy') \n",
    "X_train_2 = jnp.load('../mnist_np/X_train_2.npy') \n",
    "X_train_3 = jnp.load('../mnist_np/X_train_3.npy') \n",
    "X_train_4 = jnp.load('../mnist_np/X_train_4.npy') \n",
    "# create X_train out of 4 X_trains\n",
    "X_train = jnp.concatenate([X_train_1, X_train_2, X_train_3, X_train_4], axis=0)\n",
    "y_train = jnp.load('../mnist_np/y_train.npy')\n",
    "X_test = jnp.load('../mnist_np/X_test.npy')\n",
    "y_test = jnp.load('../mnist_np/y_test.npy')\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the data in batches of 2000 (going above take more time)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n"
     ]
    }
   ],
   "source": [
    "def rate_encoding(key, X, sim_len=100):\n",
    "    \n",
    "    def bernoulli_encoding(key, spike_trains, sim_len):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        return key, jax.random.bernoulli(key, spike_trains, (sim_len, spike_trains.shape[0], spike_trains.shape[1]))\n",
    "    \n",
    "    print('Encoding the data in batches of 2000 (going above take more time)')\n",
    "    X_encoded = []\n",
    "    batch_size = 2000\n",
    "    for i in range(X.shape[0]//batch_size):\n",
    "        key, X_encoded_ = bernoulli_encoding(key, X[i*batch_size:(i+1)*batch_size], sim_len=100)\n",
    "        print(X_encoded_.shape)\n",
    "        X_encoded.append(X_encoded_)\n",
    "\n",
    "    return key, jnp.concatenate(X_encoded, axis=1)\n",
    "\n",
    "# do rate encoding on X_test\n",
    "key = jax.random.PRNGKey(9)\n",
    "key, X_test_encoded = rate_encoding(key, X_test, sim_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the data in batches of 2000 (going above take more time)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n",
      "(100, 2000, 784)\n"
     ]
    }
   ],
   "source": [
    "# do rate encoding on Xtrain\n",
    "key, X_train_encoded = rate_encoding(key, X_train, sim_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.custom_jvp\n",
    "def gr_than(x, thr):\n",
    "    return (x > thr).astype(jnp.float32)\n",
    "\n",
    "@gr_than.defjvp\n",
    "def gr_jvp(primals, tangents):\n",
    "    x, thr = primals\n",
    "    x_dot, thr_dot = tangents\n",
    "    primal_out = gr_than(x, thr)\n",
    "    tangent_out = x_dot * 1 / (jnp.absolute(x-thr)+1)**2\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "def lif_forward(state, input_spikes):\n",
    "    w, out_spikes, I_in, V_mem = state[0]\n",
    "    tau_mem, Vth, timestep = state[1]\n",
    "    I_in = jnp.dot(w, input_spikes)\n",
    "    V_mem = (1 - timestep/tau_mem) * V_mem + I_in - out_spikes * Vth\n",
    "    # constraining V_mem to be non-negative\n",
    "    V_mem = jnp.maximum(0, V_mem)\n",
    "    out_spikes = gr_than(V_mem, Vth)\n",
    "\n",
    "    # return state\n",
    "    return ((w, out_spikes, I_in, V_mem), state[1]), (I_in, V_mem, out_spikes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomWeightInit(parent_key, scale, in_width, out_width):\n",
    "    in_width = in_width\n",
    "    out_width = out_width\n",
    "    weight_key, bias_key = jax.random.split(parent_key)\n",
    "    W = scale*jax.random.normal(weight_key, shape=(out_width, in_width)) # random init of [weights, biases] tuple for each layer\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_loss(W, static_params, img, lbl):\n",
    "    num_classes = W.shape[0]\n",
    "    V_mem = jnp.zeros((num_classes,), dtype='float32')\n",
    "    I_in = jnp.zeros((num_classes,), dtype='float32')\n",
    "    out_spikes = jnp.zeros((num_classes,), dtype='float32')\n",
    "    state = ((W, out_spikes, I_in, V_mem), static_params)\n",
    "    state, plot_values = jax.lax.scan(lif_forward,state,img)\n",
    "    V_mem_data = plot_values[1]\n",
    "    max_per_class = V_mem_data.max(axis=0)\n",
    "    prediction = max_per_class.argmax()\n",
    "    logits = jax.nn.softmax(max_per_class)\n",
    "    loss = -jnp.mean(jnp.log(logits[lbl.astype(jnp.uint8)]))\n",
    "    # accuracy\n",
    "    acc = jnp.where(prediction == lbl, 1.0, 0.0)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: implement loss_fn_vmap using jax.vmap to call mini_loss in parallel\n",
    "# NOTE: in_axes might be useful here\n",
    "def loss_fn_vmap(W, static_params, img_b, lbl_b):\n",
    "    # TODO 1.1: initialize variables\n",
    "    batch_size = img_b.shape[0]\n",
    "    local_loss = jnp.zeros(batch_size)\n",
    "    local_acc = jnp.zeros(batch_size)\n",
    "    # TODO 1.2: use vmap here, calling mini_loss\n",
    "    local_loss, local_acc = jax.vmap(mini_loss, in_axes=(None, None, 0, 0))(W, static_params, img_b, lbl_b)\n",
    "    # TODO 1.3: return average loss and accuracy\n",
    "    return local_loss.mean(), local_acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset():\n",
    "    def __init__(self, X_images,Y_labels):\n",
    "        self.imgs = X_images\n",
    "        self.lbls = Y_labels\n",
    "    def __len__(self): # return length of dataset, i.e. nb of MNIST pictures\n",
    "        return self.imgs.shape[1]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[:,idx,:], self.lbls[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test datasets\n",
    "train_dataset = MNISTDataset(X_train_encoded, y_train)\n",
    "test_dataset = MNISTDataset(X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    transposed_data = list(zip(*batch)) # *() unpacks data !\n",
    "\n",
    "    labels= np.array(transposed_data[1])\n",
    "    imgs = np.stack(transposed_data[0])\n",
    "\n",
    "    return imgs, labels\n",
    " \n",
    "# data loader from pytorch\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# initialize state variables of LIF neuron for every time step\n",
    "\n",
    "# initialize dynamic params\n",
    "seed = 9\n",
    "parent_key = jax.random.PRNGKey(seed)\n",
    "W = randomWeightInit(parent_key, 0.03, 784, 10)\n",
    "W_init = W\n",
    "\n",
    "# initalize static params\n",
    "tau_mem = 10e-3\n",
    "V_th = 1.0\n",
    "timestep = 1e-3\n",
    "\n",
    "static_params = (tau_mem, V_th, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_updates: 468, n_updates_lr: 15, transition_steps: 31.0\n"
     ]
    }
   ],
   "source": [
    "# optimizer with scheduler\n",
    "\n",
    "start_learning_rate = 1e-3\n",
    "n_epochs = 1\n",
    "hp = static_params\n",
    "w = W\n",
    "\n",
    "n_batches = len(train_loader)\n",
    "n_updates = n_epochs * n_batches\n",
    "n_updates_lr = 15\n",
    "transition_steps = np.floor(n_updates / n_updates_lr)\n",
    "print(f'n_updates: {n_updates}, n_updates_lr: {n_updates_lr}, transition_steps: {transition_steps}')\n",
    "\n",
    "# Exponential decay of the learning rate.\n",
    "scheduler = optax.exponential_decay(\n",
    "    init_value=start_learning_rate,\n",
    "    transition_steps=transition_steps,\n",
    "    decay_rate=0.99)\n",
    "\n",
    "# Combining gradient transforms using `optax.chain`.\n",
    "gradient_transform = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = gradient_transform.init(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # TODO 2.1: initialize epoch_loss and epoch_acc using jnp.zeros\n",
    "    epoch_loss = jnp.zeros(len(train_loader))\n",
    "    epoch_acc = jnp.zeros(len(train_loader))\n",
    "    # continue training for one whole epoch using mini loss\n",
    "    for batch_cnt, (img_batch, lbl_batch) in enumerate(train_loader):\n",
    "      # TODO 2.2: use jax.value_and_grad to get loss and gradient\n",
    "      (batch_loss, batch_acc), weight_grad = jax.value_and_grad(loss_fn_vmap, has_aux=True)(W, static_params, img_batch, lbl_batch)\n",
    "      \n",
    "      # TODO 2.3: use gradient_transform to update weights\n",
    "      updates, opt_state = gradient_transform.update(weight_grad, opt_state)\n",
    "      W = optax.apply_updates(W, updates)\n",
    "\n",
    "      # logging\n",
    "      if batch_cnt % 25 == 0:\n",
    "          print('   batch_cnt ', batch_cnt, ', b loss ', batch_loss, ', b accuracy ', batch_acc)\n",
    "      \n",
    "      # TODO 2.4: update epoch_loss and epoch_acc using at and set\n",
    "      epoch_loss = epoch_loss.at[batch_cnt].set(batch_loss)\n",
    "      epoch_acc = epoch_acc.at[batch_cnt].set(batch_acc)\n",
    "\n",
    "    # TODO 2.5: average epoch_loss and epoch_acc\n",
    "    epoch_loss = epoch_loss.mean()\n",
    "    epoch_acc = epoch_acc.mean()\n",
    "\n",
    "    print('')\n",
    "    print('epoch ', epoch, ', e loss ', epoch_loss, ', e acc', epoch_acc)\n",
    "    print('')\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
