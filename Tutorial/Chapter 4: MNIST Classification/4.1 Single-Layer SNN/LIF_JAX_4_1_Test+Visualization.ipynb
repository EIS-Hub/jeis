{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "px = 1/plt.rcParams[\"figure.dpi\"]\n",
    "\n",
    "import os\n",
    "\n",
    "# set how much memory in your local CPU/GPU will be pre-allocated for JAX\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".75\"\n",
    "\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset, already flat and normalized\n",
    "X_train_1 = jnp.load('../mnist_np/X_train_1.npy') \n",
    "X_train_2 = jnp.load('../mnist_np/X_train_2.npy') \n",
    "X_train_3 = jnp.load('../mnist_np/X_train_3.npy') \n",
    "X_train_4 = jnp.load('../mnist_np/X_train_4.npy') \n",
    "# create X_train out of 4 X_trains\n",
    "X_train = jnp.concatenate([X_train_1, X_train_2, X_train_3, X_train_4], axis=0)\n",
    "y_train = jnp.load('../mnist_np/y_train.npy')\n",
    "X_test = jnp.load('../mnist_np/X_test.npy')\n",
    "y_test = jnp.load('../mnist_np/y_test.npy')\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_encoding(key, X, sim_len=100):\n",
    "    \n",
    "    def bernoulli_encoding(key, spike_trains, sim_len):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        return key, jax.random.bernoulli(key, spike_trains, (sim_len, spike_trains.shape[0], spike_trains.shape[1]))\n",
    "    \n",
    "    print('Encoding the data in batches of 2000 (going above take more time)')\n",
    "    X_encoded = []\n",
    "    batch_size = 2000\n",
    "    for i in range(X.shape[0]//batch_size):\n",
    "        key, X_encoded_ = bernoulli_encoding(key, X[i*batch_size:(i+1)*batch_size], sim_len=100)\n",
    "        print(X_encoded_.shape)\n",
    "        X_encoded.append(X_encoded_)\n",
    "\n",
    "    return key, jnp.concatenate(X_encoded, axis=1)\n",
    "\n",
    "# do rate encoding on X_test\n",
    "key = jax.random.PRNGKey(9)\n",
    "key, X_test_encoded = rate_encoding(key, X_test, sim_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do rate encoding on Xtrain\n",
    "key, X_train_encoded = rate_encoding(key, X_train, sim_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.custom_jvp\n",
    "def gr_than(x, thr):\n",
    "    return (x > thr).astype(jnp.float32)\n",
    "\n",
    "@gr_than.defjvp\n",
    "def gr_jvp(primals, tangents):\n",
    "    x, thr = primals\n",
    "    x_dot, thr_dot = tangents\n",
    "    primal_out = gr_than(x, thr)\n",
    "    tangent_out = x_dot * 1 / (jnp.absolute(x-thr)+1)**2\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "def lif_forward(state, input_spikes):\n",
    "    w, out_spikes, I_in, V_mem = state[0]\n",
    "    tau_mem, Vth, timestep = state[1]\n",
    "    I_in = jnp.dot(w, input_spikes)\n",
    "    V_mem = (1 - timestep/tau_mem) * V_mem + I_in - out_spikes * Vth\n",
    "    # constraining V_mem to be non-negative\n",
    "    V_mem = jnp.maximum(0, V_mem)\n",
    "    out_spikes = gr_than(V_mem, Vth)\n",
    "\n",
    "    # return state\n",
    "    return ((w, out_spikes, I_in, V_mem), state[1]), (I_in, V_mem, out_spikes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomWeightInit(parent_key, scale, in_width, out_width):\n",
    "    in_width = in_width\n",
    "    out_width = out_width\n",
    "    weight_key, bias_key = jax.random.split(parent_key)\n",
    "    W = scale*jax.random.normal(weight_key, shape=(out_width, in_width)) # random init of [weights, biases] tuple for each layer\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_loss(W, static_params, img, lbl):\n",
    "    num_classes = W.shape[0]\n",
    "    V_mem = jnp.zeros((num_classes,), dtype='float32')\n",
    "    I_in = jnp.zeros((num_classes,), dtype='float32')\n",
    "    out_spikes = jnp.zeros((num_classes,), dtype='float32')\n",
    "    \n",
    "    state = ((W, out_spikes, I_in, V_mem), static_params)\n",
    "    state, plot_values = jax.lax.scan(lif_forward,state,img)\n",
    "    V_mem_data = plot_values[1]\n",
    "    max_per_class = V_mem_data.max(axis=0)\n",
    "    prediction = max_per_class.argmax()\n",
    "\n",
    "    logits = jax.nn.softmax(max_per_class)\n",
    "    loss = -jnp.mean(jnp.log(logits[lbl.astype(jnp.uint8)]))\n",
    "    acc = jnp.where(prediction == lbl, 1.0, 0.0)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss fucntion for version 3 below using VMAP\n",
    "def loss_fn_vmap(W, static_params, img_b, lbl_b):\n",
    "    batch_size = img_b.shape[0]\n",
    "    local_loss = jnp.zeros(batch_size)\n",
    "    local_acc = jnp.zeros(batch_size)\n",
    "    # vmap here\n",
    "    local_loss, local_acc = jax.vmap(mini_loss, in_axes=(None, None, 0, 0))(W, static_params, img_b, lbl_b)\n",
    "    return local_loss.mean(), local_acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset():\n",
    "    def __init__(self, X_images,Y_labels):\n",
    "        self.imgs = X_images\n",
    "        self.lbls = Y_labels\n",
    "    def __len__(self): # return length of dataset, i.e. nb of MNIST pictures\n",
    "        return self.imgs.shape[1]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[:,idx,:], self.lbls[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset_unencoded():\n",
    "    def __init__(self, X_images,Y_labels):\n",
    "        self.imgs = X_images\n",
    "        self.lbls = Y_labels\n",
    "    def __len__(self): # return length of dataset, i.e. nb of MNIST pictures\n",
    "        return self.lbls.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.lbls[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test functionality of MNISTDataset() class \n",
    "train_dataset = MNISTDataset(X_train_encoded, y_train)\n",
    "test_dataset = MNISTDataset(X_test_encoded, y_test)\n",
    "\n",
    "train_dataset_unencoded = MNISTDataset_unencoded(X_train, y_train)\n",
    "test_dataset_unencoded = MNISTDataset_unencoded(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    transposed_data = list(zip(*batch)) # *() unpacks data !\n",
    "\n",
    "    labels= np.array(transposed_data[1])\n",
    "    imgs = np.stack(transposed_data[0])\n",
    "\n",
    "    return imgs, labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)\n",
    "\n",
    "train_loader_unencoded = DataLoader(train_dataset_unencoded, batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "test_loader_unencoded = DataLoader(test_dataset_unencoded, batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# initialize dynamic params\n",
    "seed = 9\n",
    "parent_key = jax.random.PRNGKey(seed)\n",
    "W = randomWeightInit(parent_key, 0.03, 784, 10)\n",
    "\n",
    "# initalize static params\n",
    "tau_mem = 10e-3\n",
    "V_th = 1.0\n",
    "timestep = 1e-3\n",
    "\n",
    "static_params = (tau_mem, V_th, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying new optimizer with scheduler\n",
    "\n",
    "start_learning_rate = 1e-3\n",
    "n_epochs = 1\n",
    "hp = static_params\n",
    "w = W\n",
    "\n",
    "n_batches = len(train_loader)\n",
    "n_updates = n_epochs * n_batches\n",
    "n_updates_lr = 15\n",
    "transition_steps = np.floor(n_updates / n_updates_lr)\n",
    "print(f'n_updates: {n_updates}, n_updates_lr: {n_updates_lr}, transition_steps: {transition_steps}')\n",
    "\n",
    "# Exponential decay of the learning rate.\n",
    "scheduler = optax.exponential_decay(\n",
    "    init_value=start_learning_rate,\n",
    "    transition_steps=transition_steps,\n",
    "    decay_rate=0.99)\n",
    "\n",
    "# Combining gradient transforms using `optax.chain`.\n",
    "gradient_transform = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = gradient_transform.init(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "best_acc = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = jnp.zeros(len(train_loader))\n",
    "    epoch_acc = jnp.zeros(len(train_loader))\n",
    "    # continue training for one whole epoch using mini loss\n",
    "    for batch_cnt, (img_batch, lbl_batch) in enumerate(train_loader):\n",
    "      (batch_loss, batch_acc), weight_grad = jax.value_and_grad(loss_fn_vmap, has_aux=True)(W, img_batch, lbl_batch)\n",
    "      updates, opt_state = gradient_transform.update(weight_grad, opt_state)\n",
    "      W = optax.apply_updates(W, updates)\n",
    "\n",
    "      # logging\n",
    "      if batch_cnt % 25 == 0:\n",
    "          print('   batch_cnt ', batch_cnt, ', b loss ', batch_loss, ', b accuracy ', batch_acc)\n",
    "\n",
    "      epoch_loss = epoch_loss.at[batch_cnt].set(batch_loss)\n",
    "      epoch_acc = epoch_acc.at[batch_cnt].set(batch_acc)\n",
    "\n",
    "    epoch_loss = epoch_loss.mean()\n",
    "    epoch_acc = epoch_acc.mean()\n",
    "\n",
    "    # save best weights\n",
    "    if epoch_acc > best_acc:\n",
    "        W_final = W\n",
    "        best_acc = epoch_acc\n",
    "        print('W saved')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "    print('epoch ', epoch, ', e loss ', epoch_loss, ', e acc', epoch_acc)\n",
    "    print('')\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing SNN on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement testing one epoch on test_loader\n",
    "# NOTE: print out batch_cnt, batch_loss, batch_acc, epoch, epoch_loss, epoch_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting 3 test images more easily\n",
    "def V_mem_from_img_id(id):\n",
    "    # get V_mem_data for one test image\n",
    "    img_b_test, lbl_b_test = next(iter(test_loader_unencoded))\n",
    "\n",
    "    img_b_test_e, lbl_b_test_e = next(iter(test_loader))\n",
    "\n",
    "    img_test = img_b_test[id]\n",
    "    lbl_test = lbl_b_test[id]\n",
    "\n",
    "\n",
    "    # encode\n",
    "    key = jax.random.PRNGKey(9)\n",
    "\n",
    "    img_test = img_test*1.0\n",
    "\n",
    "    #def bernoulli_encoding(key, spike_trains, sim_len):\n",
    "    #    key, subkey = jax.random.split(key)\n",
    "    #    return key, jax.random.bernoulli(key, spike_trains, (sim_len, spike_trains.shape[0], spike_trains.shape[1]))\n",
    "\n",
    "    img_test_encoded = jax.random.bernoulli(key, img_test, (100,784))\n",
    "\n",
    "    #print(img_test_encoded.shape)\n",
    "\n",
    "    # initialize dynamic params \n",
    "    V_mem = jnp.zeros((num_classes,), dtype='float32')\n",
    "    I_in = jnp.zeros((num_classes,), dtype='float32')\n",
    "    out_spikes = jnp.zeros((num_classes,), dtype='float32')\n",
    "\n",
    "    params_dynamic = (W_final, out_spikes, I_in, V_mem)\n",
    "\n",
    "    # initalize static params\n",
    "    tau_mem = 10e-3\n",
    "    #V_th = 1.0\n",
    "    #timestep = 1e-3\n",
    "\n",
    "    params_static = (tau_mem, V_th, timestep)\n",
    "\n",
    "    # initialize state\n",
    "    state = (params_dynamic, params_static)\n",
    "    state, plot_values = jax.lax.scan(lif_forward, state, img_test_encoded)   \n",
    "    V_mem_data = plot_values[1] # shape = (100,10)\n",
    "    max_per_class = V_mem_data.max(axis=0)\n",
    "    prediction_test = max_per_class.argmax()\n",
    "\n",
    "    print(f'Label: {lbl_test} , Prediction : {prediction_test}')\n",
    "\n",
    "    return V_mem_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get V_mem_data for one test image\n",
    "img_b_test, lbl_b_test = next(iter(test_loader_unencoded))\n",
    "\n",
    "img_b_test_e, lbl_b_test_e = next(iter(test_loader))\n",
    "\n",
    "id1 = 100\n",
    "id2 = 50\n",
    "id3 = 24\n",
    "\n",
    "V_mem_data1 = V_mem_from_img_id(id1)\n",
    "V_mem_data2 = V_mem_from_img_id(id2)\n",
    "V_mem_data3 = V_mem_from_img_id(id3)\n",
    "\n",
    "test_img1 = img_b_test[id1]\n",
    "test_img2 = img_b_test[id2]\n",
    "test_img3 = img_b_test[id3]\n",
    "\n",
    "# for all V_mem_data, plot the 10 diferent membrane potentials in a single plot and next to it plot the corresponding input image\n",
    "fig, ax = plt.subplots(3, 2, figsize=(1500*px, 2500*px))\n",
    "xs = jnp.arange(100)\n",
    "y_max = V_mem_data1.max(axis=0).max(axis=0) if V_mem_data1.max(axis=0).max(axis=0) > 1.0 else 1.0\n",
    "for i in range(10):\n",
    "    ax[0, 0].plot(xs, V_mem_data1[:, i], label=f'Neuron {i}', marker='o', markersize=1)\n",
    "    ax[1, 0].plot(xs, V_mem_data2[:, i], label=f'Neuron {i}', marker='o', markersize=1)\n",
    "    ax[2, 0].plot(xs, V_mem_data3[:, i], label=f'Neuron {i}', marker='o', markersize=1)\n",
    "ax[0, 0].set_xlabel('Time step')\n",
    "ax[0, 0].set_ylabel('Membrane potential')\n",
    "ax[0, 0].set_xlim(0, 100)\n",
    "ax[0, 0].set_ylim(0, y_max)\n",
    "# legend for every plot\n",
    "ax[0, 0].legend()\n",
    "ax[1, 0].legend()\n",
    "ax[2, 0].legend()\n",
    "ax[0, 1].imshow(test_img1.reshape(28,28), cmap='gray')\n",
    "ax[1, 1].imshow(test_img2.reshape(28,28), cmap='gray')\n",
    "ax[2, 1].imshow(test_img3.reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
