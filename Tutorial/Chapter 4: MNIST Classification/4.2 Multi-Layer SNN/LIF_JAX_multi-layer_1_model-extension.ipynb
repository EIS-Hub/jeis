{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import optax\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".5\"\n",
    "px = 1/plt.rcParams[\"figure.dpi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset, already flat and normalized\n",
    "X_train_1 = jnp.load('../mnist_np/X_train_1.npy') \n",
    "X_train_2 = jnp.load('../mnist_np/X_train_2.npy') \n",
    "X_train_3 = jnp.load('../mnist_np/X_train_3.npy') \n",
    "X_train_4 = jnp.load('../mnist_np/X_train_4.npy') \n",
    "# create X_train out of 4 X_trains\n",
    "X_train = jnp.concatenate([X_train_1, X_train_2, X_train_3, X_train_4], axis=0)\n",
    "y_train = jnp.load('../mnist_np/y_train.npy')\n",
    "X_test = jnp.load('../mnist_np/X_test.npy')\n",
    "y_test = jnp.load('../mnist_np/y_test.npy')\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_encoding(key, X, sim_len=100):\n",
    "    \n",
    "    def bernoulli_encoding(key, spike_trains, sim_len):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        return key, jax.random.bernoulli(key, spike_trains, (sim_len, spike_trains.shape[0], spike_trains.shape[1]))\n",
    "    \n",
    "    print('Encoding the data in batches of 2000 (going above take more time)')\n",
    "    X_encoded = []\n",
    "    batch_size = 2000\n",
    "    for i in range(X.shape[0]//batch_size):\n",
    "        key, X_encoded_ = bernoulli_encoding(key, X[i*batch_size:(i+1)*batch_size], sim_len=100)\n",
    "        print(X_encoded_.shape)\n",
    "        X_encoded.append(X_encoded_)\n",
    "\n",
    "    return key, jnp.concatenate(X_encoded, axis=1)\n",
    "\n",
    "# do rate encoding on X_test\n",
    "key = jax.random.PRNGKey(9)\n",
    "key, X_test_encoded = rate_encoding(key, X_test, sim_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do rate encoding on Xtrain\n",
    "key, X_train_encoded = rate_encoding(key, X_train, sim_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.custom_jvp\n",
    "def gr_than(x, thr):\n",
    "    return (x > thr).astype(jnp.float32)\n",
    "\n",
    "@gr_than.defjvp\n",
    "def gr_jvp(primals, tangents):\n",
    "    x, thr = primals\n",
    "    x_dot, thr_dot = tangents\n",
    "    primal_out = gr_than(x, thr)\n",
    "    tangent_out = x_dot * 1 / (jnp.absolute(x-thr)+1)**2\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "# TODO 1: extend the LIF model to have two layers\n",
    "# NOTE: nb of inputs and outputs are the same, the hidden layer dimension has to be defined\n",
    "# NOTE: a recurrent layer is defined at the hidden layer\n",
    "# NOTE: we want to return besides the state also the input current, V_mem of the hidden layer, V_mem and the output spikes of the output layer\n",
    "def lif_forward(state, input_spikes):\n",
    "    \n",
    "    return ((None, None, None, None, None, None), state[1]), (I_in, V_mem1, V_mem2, out_spikes2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomWeightInit(parent_key, scale, in_width, out_width):\n",
    "    in_width = in_width\n",
    "    out_width = out_width\n",
    "    weight_key, bias_key = jax.random.split(parent_key)\n",
    "    W = scale*jax.random.normal(weight_key, shape=(out_width, in_width))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: implement a multi-layer loss function\n",
    "def mini_loss(params, static_params, img, lbl):\n",
    "    # TODO 3.1: try to infer these values from your parameters\n",
    "    num_classes = None\n",
    "    num_hidden = None\n",
    "\n",
    "    # TODO 3.2: initialize the multi-layer state variable\n",
    "    state = None\n",
    "\n",
    "    state, plot_values = jax.lax.scan(lif_forward,state,img)\n",
    "\n",
    "    # TODO 3.3: implement the loss calulcation accordingly\n",
    "    V_mem_data = None\n",
    "    max_per_class = None\n",
    "    prediction = None\n",
    "    logits = None\n",
    "    loss = None\n",
    "    acc = None\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss_fn_vmap(params, static_params, img_b, lbl_b):\n",
    "    batch_size = img_b.shape[0]\n",
    "    \n",
    "    local_loss = jnp.zeros(batch_size)\n",
    "    local_acc = jnp.zeros(batch_size)\n",
    "\n",
    "    local_loss, local_acc = jax.vmap(mini_loss, in_axes=(None, None, 0, 0))(params, static_params, img_b, lbl_b)\n",
    "    return local_loss.mean(), local_acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset():\n",
    "    def __init__(self, X_images,Y_labels):\n",
    "        self.imgs = X_images\n",
    "        self.lbls = Y_labels\n",
    "    def __len__(self): \n",
    "        return self.imgs.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[:,idx,:], self.lbls[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test functionality of MNISTDataset() class \n",
    "train_dataset = MNISTDataset(X_train_encoded, y_train)\n",
    "test_dataset = MNISTDataset(X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    transposed_data = list(zip(*batch))\n",
    "    labels= np.array(transposed_data[1])\n",
    "    imgs = np.stack(transposed_data[0])\n",
    "    return imgs, labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "# TODO LATER: play around, try out different hidden layer dimensions and check out the resulting accuracy\n",
    "# NOTE: this one works sufficiently well\n",
    "num_hidden = 200\n",
    "\n",
    "# initialize state variables of LIF neuron for every time step\n",
    "\n",
    "# initialize dynamic params\n",
    "\n",
    "# TODO 2: modify/add parameters necessary for a multi-layer SNN\n",
    "\n",
    "seed = 9\n",
    "parent_key = jax.random.PRNGKey(seed)\n",
    "\n",
    "# TODO 2.1: initialize weights\n",
    "# NOTE: weights can be added to a list/tuple of weights (called params for instance), and passed as an argument\n",
    "# NOTE: weight scaling factor of 0.03 can be used\n",
    "# NOTE: refer to the JAX tutorial on weight updates with JAX using Pytrees to see why we are using it this way\n",
    "params = None\n",
    "params_init = params\n",
    "\n",
    "tau_mem = 10e-3\n",
    "V_th = 1.0\n",
    "timestep = 1e-3\n",
    "\n",
    "static_params = (tau_mem, V_th, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying new optimizer with scheduler\n",
    "\n",
    "start_learning_rate = 1e-3\n",
    "n_epochs = 1\n",
    "hp = static_params\n",
    "\n",
    "n_batches = len(train_loader)\n",
    "n_updates = n_epochs * n_batches\n",
    "n_updates_lr = 15\n",
    "transition_steps = np.floor(n_updates / n_updates_lr)\n",
    "print(f'n_updates: {n_updates}, n_updates_lr: {n_updates_lr}, transition_steps: {transition_steps}')\n",
    "\n",
    "# Exponential decay of the learning rate.\n",
    "scheduler = optax.exponential_decay(\n",
    "    init_value=start_learning_rate,\n",
    "    transition_steps=transition_steps,\n",
    "    decay_rate=0.99)\n",
    "\n",
    "# Combining gradient transforms using `optax.chain`.\n",
    "gradient_transform = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = gradient_transform.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "best_acc = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = jnp.zeros(len(train_loader))\n",
    "    epoch_acc = jnp.zeros(len(train_loader))\n",
    "    # continue training for one whole epoch using mini loss\n",
    "    for batch_cnt, (img_batch, lbl_batch) in enumerate(train_loader):\n",
    "      (batch_loss, batch_acc), weight_grad = jax.value_and_grad(loss_fn_vmap, has_aux=True)(params, static_params, img_batch, lbl_batch)\n",
    "      updates, opt_state = gradient_transform.update(weight_grad, opt_state)\n",
    "      params = optax.apply_updates(params, updates)\n",
    "\n",
    "      # logging\n",
    "      if batch_cnt % 25 == 0:\n",
    "          print('   batch_cnt ', batch_cnt, ', b loss ', batch_loss, ', b accuracy ', batch_acc)\n",
    "\n",
    "      epoch_loss = epoch_loss.at[batch_cnt].set(batch_loss)\n",
    "      epoch_acc = epoch_acc.at[batch_cnt].set(batch_acc)\n",
    "\n",
    "    epoch_loss = epoch_loss.mean()\n",
    "    epoch_acc = epoch_acc.mean()\n",
    "\n",
    "    print('')\n",
    "    print('epoch ', epoch, ', e loss ', epoch_loss, ', e acc', epoch_acc)\n",
    "    print('')\n",
    "\n",
    "    # save best performing weight (per epoch)\n",
    "    if epoch_acc > best_acc:\n",
    "      # TODO 2.3: save the best performing weights into params_final\n",
    "      params_final = None\n",
    "      best_acc = epoch_acc\n",
    "      print('params saved')\n",
    "      print('')\n",
    "\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing SNN on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "batch_img, batch_lbl = next(iter(test_loader))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = jnp.zeros(len(test_loader))\n",
    "    epoch_acc = jnp.zeros(len(test_loader))\n",
    "    # continue training for one whole epoch using mini loss\n",
    "    for batch_cnt, (img_batch, lbl_batch) in enumerate(test_loader):\n",
    "\n",
    "      # simple inference using W_final\n",
    "      batch_loss, batch_acc = loss_fn_vmap(params_final, static_params, img_batch, lbl_batch)\n",
    "\n",
    "      # logging\n",
    "      if batch_cnt % 25 == 0:\n",
    "          print('   batch_cnt ', batch_cnt, ', b loss ', batch_loss, ', b accuracy ', batch_acc)\n",
    "\n",
    "      epoch_loss = epoch_loss.at[batch_cnt].set(batch_loss)\n",
    "      epoch_acc = epoch_acc.at[batch_cnt].set(batch_acc)\n",
    "\n",
    "    epoch_loss = epoch_loss.mean()\n",
    "    epoch_acc = epoch_acc.mean()\n",
    "\n",
    "    print('')\n",
    "    print('epoch ', epoch, ', e loss ', epoch_loss, ', e acc', epoch_acc)\n",
    "    print('')\n",
    "\n",
    "\n",
    "print('DONE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
