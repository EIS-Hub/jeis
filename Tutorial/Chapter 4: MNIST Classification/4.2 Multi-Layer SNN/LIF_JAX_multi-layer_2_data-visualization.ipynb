{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import optax\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".5\"\n",
    "px = 1/plt.rcParams[\"figure.dpi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset, already flat and normalized\n",
    "X_train_1 = jnp.load('../mnist_np/X_train_1.npy') \n",
    "X_train_2 = jnp.load('../mnist_np/X_train_2.npy') \n",
    "X_train_3 = jnp.load('../mnist_np/X_train_3.npy') \n",
    "X_train_4 = jnp.load('../mnist_np/X_train_4.npy') \n",
    "# create X_train out of 4 X_trains\n",
    "X_train = jnp.concatenate([X_train_1, X_train_2, X_train_3, X_train_4], axis=0)\n",
    "y_train = jnp.load('../mnist_np/y_train.npy')\n",
    "X_test = jnp.load('../mnist_np/X_test.npy')\n",
    "y_test = jnp.load('../mnist_np/y_test.npy')\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_encoding(key, X, sim_len=100):\n",
    "    \n",
    "    def bernoulli_encoding(key, spike_trains, sim_len):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        return key, jax.random.bernoulli(key, spike_trains, (sim_len, spike_trains.shape[0], spike_trains.shape[1]))\n",
    "    \n",
    "    print('Encoding the data in batches of 2000 (going above take more time)')\n",
    "    X_encoded = []\n",
    "    batch_size = 2000\n",
    "    for i in range(X.shape[0]//batch_size):\n",
    "        key, X_encoded_ = bernoulli_encoding(key, X[i*batch_size:(i+1)*batch_size], sim_len=100)\n",
    "        print(X_encoded_.shape)\n",
    "        X_encoded.append(X_encoded_)\n",
    "\n",
    "    return key, jnp.concatenate(X_encoded, axis=1)\n",
    "\n",
    "# do rate encoding on X_test\n",
    "key = jax.random.PRNGKey(9)\n",
    "key, X_test_encoded = rate_encoding(key, X_test, sim_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do rate encoding on Xtrain\n",
    "key, X_train_encoded = rate_encoding(key, X_train, sim_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.custom_jvp\n",
    "def gr_than(x, thr):\n",
    "    return (x > thr).astype(jnp.float32)\n",
    "\n",
    "@gr_than.defjvp\n",
    "def gr_jvp(primals, tangents):\n",
    "    x, thr = primals\n",
    "    x_dot, thr_dot = tangents\n",
    "    primal_out = gr_than(x, thr)\n",
    "    tangent_out = x_dot * 1 / (jnp.absolute(x-thr)+1)**2\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "def lif_forward(state, input_spikes):\n",
    "    params, hidden_spikes, out_spikes2, I_in, V_mem1, V_mem2 = state[0]\n",
    "    tau_mem, Vth, timestep = state[1]\n",
    "    W01 = params[0]\n",
    "    W11 = params[1]\n",
    "    W12 = params[2]\n",
    "    \n",
    "    # Layer 1\n",
    "    I_in = jnp.dot(W01, input_spikes) # shape (8,1)\n",
    "    # recurrent connection\n",
    "    I_rec = jnp.dot(W11, hidden_spikes) # shape (8,1)\n",
    "    V_mem1 = (1 - timestep/tau_mem) * V_mem1 + I_rec + I_in - hidden_spikes * Vth\n",
    "    # constraining V_mem to be non-negative\n",
    "    V_mem1 = jnp.maximum(0, V_mem1)\n",
    "    hidden_spikes = gr_than(V_mem1, Vth)\n",
    "    # Layer 2\n",
    "    I_in2 = jnp.dot(W12, hidden_spikes)\n",
    "    V_mem2 = (1 - timestep/tau_mem) * V_mem2 + I_in2 # no reset, just integrate\n",
    "    # constraining V_mem to be non-negative\n",
    "    V_mem2 = jnp.maximum(0, V_mem2)\n",
    "    out_spikes2 = gr_than(V_mem2, Vth)\n",
    "    # return state\n",
    "    return ((params, hidden_spikes, out_spikes2, I_in, V_mem1, V_mem2), state[1]), (I_in, V_mem1, V_mem2, out_spikes2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomWeightInit(parent_key, scale, in_width, out_width):\n",
    "    in_width = in_width\n",
    "    out_width = out_width\n",
    "    weight_key, bias_key = jax.random.split(parent_key)\n",
    "    W = scale*jax.random.normal(weight_key, shape=(out_width, in_width))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_loss(params, static_params, img, lbl):\n",
    "    num_classes = params[2].shape[0]\n",
    "    num_hidden = params[1].shape[0]\n",
    "    \n",
    "    V_mem1 = jnp.zeros((num_hidden,), dtype='float32')\n",
    "    I_in = jnp.zeros((num_hidden,), dtype='float32')\n",
    "    hidden_spikes = jnp.zeros((num_hidden,), dtype='float32')\n",
    "    V_mem2 = jnp.zeros((num_classes,), dtype='float32')\n",
    "    out_spikes = jnp.zeros((num_classes,), dtype='float32')\n",
    "    state = ((params, hidden_spikes, out_spikes, I_in, V_mem1, V_mem2), static_params)\n",
    "\n",
    "    state, plot_values = jax.lax.scan(lif_forward,state,img)\n",
    "\n",
    "    V_mem_data = plot_values[2]\n",
    "    max_per_class = V_mem_data.max(axis=0)\n",
    "    prediction = max_per_class.argmax()\n",
    "    logits = jax.nn.softmax(max_per_class)\n",
    "    loss = -jnp.mean(jnp.log(logits[lbl.astype(jnp.uint8)]))\n",
    "    acc = jnp.where(prediction == lbl, 1.0, 0.0)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss_fn_vmap(params, static_params, img_b, lbl_b):\n",
    "    batch_size = img_b.shape[0]\n",
    "    \n",
    "    local_loss = jnp.zeros(batch_size)\n",
    "    local_acc = jnp.zeros(batch_size)\n",
    "\n",
    "    local_loss, local_acc = jax.vmap(mini_loss, in_axes=(None, None, 0, 0))(params, static_params, img_b, lbl_b)\n",
    "    return local_loss.mean(), local_acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset():\n",
    "    def __init__(self, X_images,Y_labels):\n",
    "        self.imgs = X_images\n",
    "        self.lbls = Y_labels\n",
    "    def __len__(self): \n",
    "        return self.imgs.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[:,idx,:], self.lbls[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset_unencoded():\n",
    "    def __init__(self, X_images,Y_labels):\n",
    "        self.imgs = X_images\n",
    "        self.lbls = Y_labels\n",
    "    def __len__(self): \n",
    "        return self.lbls.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.lbls[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNISTDataset(X_train_encoded, y_train)\n",
    "test_dataset = MNISTDataset(X_test_encoded, y_test)\n",
    "\n",
    "train_dataset_unencoded = MNISTDataset_unencoded(X_train, y_train)\n",
    "test_dataset_unencoded = MNISTDataset_unencoded(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    transposed_data = list(zip(*batch))\n",
    "    labels= np.array(transposed_data[1])\n",
    "    imgs = np.stack(transposed_data[0])\n",
    "    return imgs, labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)\n",
    "\n",
    "train_loader_unencoded = DataLoader(train_dataset_unencoded, batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "test_loader_unencoded = DataLoader(test_dataset_unencoded, batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_hidden = 200\n",
    "\n",
    "# initialize state variables of LIF neuron for every time step\n",
    "\n",
    "# initialize dynamic params\n",
    "\n",
    "seed = 9\n",
    "parent_key = jax.random.PRNGKey(seed)\n",
    "W01 = randomWeightInit(parent_key, 0.03, 784, num_hidden)\n",
    "W11 = randomWeightInit(parent_key, 0.03, num_hidden, num_hidden)\n",
    "W12 = randomWeightInit(parent_key, 0.03, num_hidden, num_classes)\n",
    "params = [W01, W11, W12]\n",
    "params_init = params\n",
    "\n",
    "tau_mem = 10e-3\n",
    "V_th = 1.0\n",
    "timestep = 1e-3\n",
    "\n",
    "static_params = (tau_mem, V_th, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer with scheduler\n",
    "\n",
    "start_learning_rate = 1e-3\n",
    "n_epochs = 1\n",
    "hp = static_params\n",
    "\n",
    "n_batches = len(train_loader)\n",
    "n_updates = n_epochs * n_batches\n",
    "n_updates_lr = 15\n",
    "transition_steps = np.floor(n_updates / n_updates_lr)\n",
    "print(f'n_updates: {n_updates}, n_updates_lr: {n_updates_lr}, transition_steps: {transition_steps}')\n",
    "\n",
    "# Exponential decay of the learning rate.\n",
    "scheduler = optax.exponential_decay(\n",
    "    init_value=start_learning_rate,\n",
    "    transition_steps=transition_steps,\n",
    "    decay_rate=0.99)\n",
    "\n",
    "# Combining gradient transforms using `optax.chain`.\n",
    "gradient_transform = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = gradient_transform.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "best_acc = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = jnp.zeros(len(train_loader))\n",
    "    epoch_acc = jnp.zeros(len(train_loader))\n",
    "    # continue training for one whole epoch using mini loss\n",
    "    for batch_cnt, (img_batch, lbl_batch) in enumerate(train_loader):\n",
    "      (batch_loss, batch_acc), weight_grad = jax.value_and_grad(loss_fn_vmap, has_aux=True)(params, static_params, img_batch, lbl_batch)\n",
    "      updates, opt_state = gradient_transform.update(weight_grad, opt_state)\n",
    "      params = optax.apply_updates(params, updates)\n",
    "\n",
    "      # logging\n",
    "      if batch_cnt % 25 == 0:\n",
    "          print('   batch_cnt ', batch_cnt, ', b loss ', batch_loss, ', b accuracy ', batch_acc)\n",
    "\n",
    "      epoch_loss = epoch_loss.at[batch_cnt].set(batch_loss)\n",
    "      epoch_acc = epoch_acc.at[batch_cnt].set(batch_acc)\n",
    "\n",
    "    epoch_loss = epoch_loss.mean()\n",
    "    epoch_acc = epoch_acc.mean()\n",
    "\n",
    "    print('')\n",
    "    print('epoch ', epoch, ', e loss ', epoch_loss, ', e acc', epoch_acc)\n",
    "    print('')\n",
    "\n",
    "    # save best performing weight (per epoch)\n",
    "    if epoch_acc > best_acc:\n",
    "      params_final = params\n",
    "      best_acc = epoch_acc\n",
    "      print('params saved')\n",
    "      print('')\n",
    "\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing SNN on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "batch_img, batch_lbl = next(iter(test_loader))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = jnp.zeros(len(test_loader))\n",
    "    epoch_acc = jnp.zeros(len(test_loader))\n",
    "    # continue training for one whole epoch using mini loss\n",
    "    for batch_cnt, (img_batch, lbl_batch) in enumerate(test_loader):\n",
    "\n",
    "      # simple inference using W_final\n",
    "      batch_loss, batch_acc = loss_fn_vmap(params_final, static_params, img_batch, lbl_batch)\n",
    "\n",
    "      # logging\n",
    "      if batch_cnt % 25 == 0:\n",
    "          print('   batch_cnt ', batch_cnt, ', b loss ', batch_loss, ', b accuracy ', batch_acc)\n",
    "\n",
    "      epoch_loss = epoch_loss.at[batch_cnt].set(batch_loss)\n",
    "      epoch_acc = epoch_acc.at[batch_cnt].set(batch_acc)\n",
    "\n",
    "    epoch_loss = epoch_loss.mean()\n",
    "    epoch_acc = epoch_acc.mean()\n",
    "\n",
    "    print('')\n",
    "    print('epoch ', epoch, ', e loss ', epoch_loss, ', e acc', epoch_acc)\n",
    "    print('')\n",
    "\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: implement function for plotting 3 test images more easily\n",
    "def V_mem_from_img_id(id, params_final, test_loader_unencoded):\n",
    "    # TODO 1.1: get V_mem_data for one test image\n",
    "    img_b_test, lbl_b_test = None, None\n",
    "\n",
    "    img_test = img_b_test[id]\n",
    "    lbl_test = lbl_b_test[id]\n",
    "\n",
    "    # TODO 1.2: define num_classes and num_hidden given params_final\n",
    "    num_classes = None\n",
    "    num_hidden = None\n",
    "\n",
    "    # encode\n",
    "    key = jax.random.PRNGKey(9)\n",
    "    img_test = img_test*1.0\n",
    "    # TODO 1.3: encode image using jax.random.bernoulli\n",
    "    img_test_encoded = None\n",
    "\n",
    "    # TODO 1.4: initialize dynamic params \n",
    "    V_mem1 = None\n",
    "    I_in = None\n",
    "    hidden_spikes = None\n",
    "    V_mem2 = None\n",
    "    out_spikes = None\n",
    "    params_dynamic = None\n",
    "    tau_mem = 10e-3\n",
    "    params_static = None\n",
    "\n",
    "    # initialize state\n",
    "    state = (params_dynamic, params_static)\n",
    "    # TODO 1.5: scan through timesteps using lif_forward and the encoded image\n",
    "    state, plot_values = None, None\n",
    "    # TODO 1.6: get test prediction\n",
    "    V_mem_data = None\n",
    "    max_per_class = None\n",
    "    prediction_test = None\n",
    "\n",
    "    print(f'Label: {lbl_test} , Prediction : {prediction_test}')\n",
    "\n",
    "    return V_mem_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get V_mem_data for one test image\n",
    "img_b_test, lbl_b_test = next(iter(test_loader_unencoded))\n",
    "img_b_test_e, lbl_b_test_e = next(iter(test_loader))\n",
    "\n",
    "id1 = 100\n",
    "id2 = 50\n",
    "id3 = 24\n",
    "\n",
    "V_mem_data1 = V_mem_from_img_id(id1, params_final, test_loader_unencoded)\n",
    "V_mem_data2 = V_mem_from_img_id(id2, params_final, test_loader_unencoded)\n",
    "V_mem_data3 = V_mem_from_img_id(id3, params_final, test_loader_unencoded)\n",
    "\n",
    "test_img1 = img_b_test[id1]\n",
    "test_img2 = img_b_test[id2]\n",
    "test_img3 = img_b_test[id3]\n",
    "\n",
    "# for all V_mem_data, plot the 10 diferent membrane potentials in a single plot and next to it plot the corresponding input image\n",
    "fig, ax = plt.subplots(3, 2, figsize=(1500*px, 2500*px))\n",
    "xs = jnp.arange(100)\n",
    "y_max = V_mem_data1.max(axis=0).max(axis=0) if V_mem_data1.max(axis=0).max(axis=0) > 1.0 else 1.0\n",
    "for i in range(10):\n",
    "    ax[0, 0].plot(xs, V_mem_data1[:, i], label=f'Neuron {i}', marker='o', markersize=1)\n",
    "    ax[1, 0].plot(xs, V_mem_data2[:, i], label=f'Neuron {i}', marker='o', markersize=1)\n",
    "    ax[2, 0].plot(xs, V_mem_data3[:, i], label=f'Neuron {i}', marker='o', markersize=1)\n",
    "ax[0, 0].set_xlabel('Time step')\n",
    "ax[0, 0].set_ylabel('Membrane potential')\n",
    "ax[0, 0].set_xlim(0, 100)\n",
    "ax[0, 0].set_ylim(0, y_max)\n",
    "# legend for every plot\n",
    "ax[0, 0].legend()\n",
    "ax[1, 0].legend()\n",
    "ax[2, 0].legend()\n",
    "ax[0, 1].imshow(test_img1.reshape(28,28), cmap='gray')\n",
    "ax[1, 1].imshow(test_img2.reshape(28,28), cmap='gray')\n",
    "ax[2, 1].imshow(test_img3.reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: implement prediction function on one image for confusion matrix\n",
    "def predict_one_img(state, img):\n",
    "    V_mem_data = []\n",
    "    # TODO 2.1: jax scan over time steps of image using lif forward\n",
    "    state, plot_values = None, None  \n",
    "    # TODO 2.2: get prediction\n",
    "    V_mem_data = None\n",
    "    max_per_class = None\n",
    "    predicted_label = None\n",
    "    return predicted_label, max_per_class\n",
    "\n",
    "v_predict_one_img = jax.vmap(predict_one_img, in_axes=(None, 0), out_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "img_b_test_e, lbl_b_test_e = next(iter(test_loader))\n",
    "\n",
    "# initialize state variables and state\n",
    "V_mem1 = jnp.zeros((num_hidden,), dtype='float32')\n",
    "I_in = jnp.zeros((num_hidden,), dtype='float32')\n",
    "hidden_spikes = jnp.zeros((num_hidden,), dtype='float32')\n",
    "V_mem2 = jnp.zeros((num_classes,), dtype='float32')\n",
    "out_spikes = jnp.zeros((num_classes,), dtype='float32')\n",
    "\n",
    "state = ((params_final, hidden_spikes, out_spikes, I_in, V_mem1, V_mem2), static_params)\n",
    "\n",
    "prediction_test, max_per_class = v_predict_one_img(state, img_b_test_e)\n",
    "\n",
    "# get the confusion matrix\n",
    "cm = confusion_matrix(lbl_b_test_e, prediction_test)\n",
    "\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(lbl_b_test_e, prediction_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
